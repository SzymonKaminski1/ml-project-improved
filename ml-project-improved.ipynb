{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('data_test.csv')\n",
    "X_train = pd.read_csv('data_train.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of variables: \", len(X_train.columns), \"\\nNumber of observations: \", len(X_train.index))\n",
    "X_train.info()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all data is numerical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum().sort_values()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data missing. Now we are separating data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_train.Y\n",
    "X_train.drop(['Y'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From prevoius cells we can see that values are not standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_std, index=X_train.index, columns=X_train.columns)\n",
    "X_train.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to see how ElasticNet model is performing on our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from heapq import nsmallest\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: from previous work on this project I noticed that ElasticNet model does not work well during the fitting strictly for Ridge/Lasso. I will use ElasticNet for \"middle\" values for parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ElasticNet\n",
    "\n",
    "def get_score_preliminary_en(alpha, l1_ratio):\n",
    "    scores = (-1)*cross_val_score(ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=1), X_train, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge\n",
    "\n",
    "def get_score_preliminary_r(alpha):\n",
    "    scores = (-1)*cross_val_score(Ridge(alpha=alpha, random_state=1), X_train, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n",
    "def get_score_preliminary_l(alpha):\n",
    "    scores = (-1)*cross_val_score(Lasso(alpha=alpha, random_state=1), X_train, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying results in intuitive way\n",
    "\n",
    "def display_hm(alphas, l1_ratio, results):\n",
    "    ax = sns.heatmap(results, linewidth=0.5, xticklabels=alphas, yticklabels=l1_ratio)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_results_arr(a):\n",
    "    \n",
    "    # \"a\" parameter is for choosing how many \"alphas\" we want\n",
    "    alphas_preliminary = [pow(10, i-2) for i in range(0, a)]\n",
    "    l1_ratio_preliminary = [i/10 for i in range(0, 11)]\n",
    "\n",
    "    #setting array and dictionary to store our results\n",
    "    results_preliminary = np.zeros((len(l1_ratio_preliminary), len(alphas_preliminary)))\n",
    "    results_preliminary_dict = {}\n",
    "\n",
    "    #traning and saving results\n",
    "    for i in range(0, len(alphas_preliminary)):\n",
    "        results_preliminary[0, i] = get_score_preliminary_r(alphas_preliminary[i])  # Ridge for l1_ratio = 0\n",
    "        results_preliminary_dict[0, i] = results_preliminary[0, i]\n",
    "\n",
    "    for i in range(0, len(alphas_preliminary)):\n",
    "        results_preliminary[10, i] = get_score_preliminary_l(alphas_preliminary[i])  # Lasso for l1_ratio = 1\n",
    "        results_preliminary_dict[10, i] = results_preliminary[10, i]\n",
    "\n",
    "    for j in range(0, len(l1_ratio_preliminary)-1):\n",
    "        for i in range(0, len(alphas_preliminary)):\n",
    "            results_preliminary[j+1, i] = get_score_preliminary_en(alphas_preliminary[i], l1_ratio_preliminary[j+1])  # ElasticNet for \"middle\" values\n",
    "            results_preliminary_dict[j+1, i] = results_preliminary[j+1, i]\n",
    "\n",
    "    # displaying results\n",
    "    display_hm(alphas_preliminary, l1_ratio_preliminary,  results_preliminary)\n",
    "    \n",
    "    return results_preliminary, results_preliminary_dict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_preliminary, results_preliminary_dict = train_save_results_arr(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nsmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallets_10 = nsmallest(10, results_preliminary_dict, key = results_preliminary_dict.get)\n",
    "for i in smallets_10:\n",
    "    print(i, results_preliminary_dict[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = min(results_preliminary_dict, key=results_preliminary_dict.get)\n",
    "mse_preliminary = results_preliminary_dict[id]\n",
    "punishment_preliminary = pow(10, id[1]-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that good candidate for best model is Ridge Regression. My idea is to find best \"punishment\" value. I am doing it by \n",
    "1. taking best punishment from preliminary training\n",
    "2. setting smaller and bigger \"logarithmic\" neigbour, I mean:\n",
    "    1. we know that in preliminary training logarithmic interval beetween alphas was \"1\".\n",
    "    2. for finding better punishment value we create new alphas values by taking values in the middle (logarithmically) of left and right current punishmet value's intervals\n",
    "3. looping this proces till the moment when mse improvement is smaller than 0.05% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_punishment_value(p_min, mse_min, log_interval):\n",
    "    log_val = math.log10(p_min)\n",
    "    new_log_interval = log_interval*0.5\n",
    "    alphas = [pow(10, log_val - new_log_interval), pow(10, log_val + new_log_interval)] #[pow(-), \"p_min,\" pow(+)]\n",
    "    results = {}\n",
    "    results[p_min] = mse_min\n",
    "    for i in alphas:\n",
    "        results[i] = get_score_preliminary_r(i)\n",
    "    \n",
    "    new_p_min = min(results, key=results.get)\n",
    "\n",
    "    if p_min == new_p_min:\n",
    "        return determine_punishment_value(p_min, mse_min, new_log_interval) #preventing from falsely marking p_min as \"best\" only because it was chosen again (improvement = 0)\n",
    "    \n",
    "    else:\n",
    "        new_mse_min = results[new_p_min]\n",
    "        improvement = (mse_min-new_mse_min)/mse_min\n",
    "        lvl = 0.005\n",
    "\n",
    "        if(improvement >= lvl):\n",
    "            return determine_punishment_value(new_p_min, new_mse_min, new_log_interval)\n",
    "        \n",
    "        else:\n",
    "            return new_p_min, new_mse_min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_punishment, best_mse = determine_punishment_value(punishment_preliminary, mse_preliminary, 1)\n",
    "print(\"Best punishment: \", best_punishment, \"\\n Best mse: \", best_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "531782c982964c65c938113cbb69c0a534a2104b0692198f1e69c6c5005775c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
